# Comparing Anomaly-Detection Algorithms for Keystroke Dynamics
Kevin S. Killourhy, Roy A. Maxion

Identifier les meilleurs algorithmes pour l'identification d'un utilisateur en 
se basant sur son rythme de frappe au clavier.

## Introduction
*Keystroke dynamics* : moyen pour diff√©rencier entre un utilisateur authentique
et un imposteur quand les deux ont acc√®s au mot de passe.

Il y a beaucoup d'algorithmes de d√©tection des anomalies, et il est naturel de
se demander lesquels sont les plus performants. Surtout qu'il existe un probl√®me
d'√©valuation entre les diff√©rentes techniques vu qu'elles s'accompagnent toutes
de leur propre protocole d'acquisition / √©valuation.

Le standard europ√©en pour les syst√®mes de contr√¥le d'acc√®s (EN-50133-1) exige un
FRR en dessous de 1 % et un *miss rate* (√©quivalent FAR) en dessous de 0.001 %
. Et pour l'instant aucun des algorithmes utilis√©s n'a atteint ce score, ce qui 
motive une publication pour d√©terminer quelle est la voie la plus prometteuse. 

## Background and related work
### Review of keystroke dynamics
Beaucoup de choses ont d√©j√† √©t√© faites.
Diff√©rences de m√©thodes dans le domaine :

* Sur quoi porte l'analyse ?
  * Analyse sur mdp
  * Analyse sur un paragraphe

* Quel type d'analyse ?
  * D√©tection d'anomalie = classification binaire (une seule classe)
  * Identification d'une personne parmi plusieur : classifiction √† plusieurs 
classes.

Les diff√©rents types d'analyse ne peuvent pas √™tre √©valu√©es suivant les m√™mes
crit√®res car les objectifs et les outputs sont tr√®s diff√©rents.

### <a name="Anomaly_detectors">Anomaly detectors for password timing</a>

La publication va se concentrer sur la **d√©tection d'imposteur sur mot de 
passe.**. L'inventaire des publications ayant port√© exactement sur ce sujet 
soul√®ve la disparit√© qui existe entre les m√©thodes utilis√©es, sans m√™me parler 
des algos en eux-m√™mes.

Sans surprise, la qualit√© des donn√©es a une influence tr√®s importante sur les 
performances.


**Le tableau est plut√¥t sympa pour savoir quelles *features* utiliser, quels
sont les r√©sultats, √† quoi correspondent les *features*, etc.**

Les sections int√©ressantes sont : **Features Sets** -> description des 
*features* courantes et √† quoi elles correspondent. **Results Threshold** ->
comment le seuil a √©t√© fix√© sur les *miss and false alarm rate*.

## Problem and approach
Pas possible d'√©valuer les r√©sultats des publications sur la base de ce qu'ils
rapportent dans leurs protocoles d'√©valuation, il y a trop de variations
d'une exp√©rience √† l'autre. Cette publication se concentre sur la production
d'un jeu de donn√©es et d'une proc√©dure d'√©valuation commune pour les algorithmes
de d√©tection des imposteurs.

## Password-data collection
### Choosing a password
Choix d'un mdp al√©atoire sur 10 caract√®res.

### Data-collection apparatus
Ordi portable + clavier externe. Application Windows qui effectue la capture.
L'application ne valide que les frappes correctes et se d√©barrasse des autres.
Pour la pr√©cision des timestamps, le logiciel utilise une horloge de r√©f√©rence
externe.

Sessions de 50 samples par utilisateur : permet une grande coh√©rence des donn√©es
√âviter d'avoir un jeu de donn√©es qui produit des particularit√©s.

> If some subjects typed the password more frequently than others, or if 
> different subjects used differend keyboards, these differences could make it
> artificially easier for an anomaly detector to distinguish between typists.

### Running subjects
Description de la population de test. Chaque sujet a fait 8 sessions de 50 pw,
pour un total de 400 mdp, avec des variations entre les sessions.

### Extracting timing vectors
Extraction des *features* de la donn√©e de frappe dans un *timing vector*. Pour
chaque mdp de 11 frappes (10 caract√®res + entr√©e), 31 features temporelles sont
extraites et ins√©r√©es dans un *timing vector*. Les donn√©es qui peuvent √™tre
corr√©l√©es n'ont pas √©t√© r√©duites.

## Detector implementation
Impl√©mentation la plus fid√®le possible des diff√©rents algos de d√©tection 
d'imposteurs d√©crits dans le tableau de la section 
[Anomaly detectors for password timing](#Anomaly_detectors)

Chaque d√©tecteur subit une phase d'entra√Ænement bas√©e sur un ensemble de
vecteurs de timing associ√©s √† un utilisateur, puis sune phase de test avec des
nouvelles donn√©es auxquel le d√©tecteur associe un score d'anomalie (taux de
d√©tection d'un imposteur).

La pond√©ration des param√®tres s'est faite suivant les publications d'origine
des d√©tecteurs reproduits. Le probl√®me de la pond√©ration des param√®tres est
qu'elle introduit tout de suite un biais qui favorise la m√©thode pour laquelle
elle avait √©t√© con√ßue.

**Liste des diff√©rentes impl√©mentations avec une description concise de chaque
algorithme** :

### Euclidean
Calcul du vecteur moyen du nuage de points du set d'entrainement, puis anomalie
= (distance euclidienne entre le vecteur test√© et le vecteur moyen) au carr√©.

### Euclidean (normed)
aka "normalized minimum distance classifier". Pareil que pr√©cedemment sauf qu'on
normalise la distance en la divisant par le produit de la norme des deux 
vecteurs.

### Manhattan
On calcule le vecteur moyen en phase d'entrainement, puis distance de Manhattan
entre le vecteur test√© et le vecteur moyen.
[Distance de Manhattan](https://en.wikipedia.org/wiki/Taxicab_geometry)

### Manhattan (filtered)
On a filtr√© les *outliers* de la donn√©e d'entrainement avant de faire le vecteur
moyen (on fait une premi√®re moyenne, pui son calcule la d√©viation standard, tout
point au dessus de 3 d√©viations standard est consid√©r√© comme un *outlier* puis
est vir√© du set, et on recalcule une meilleure moyenne).

### Manhattan (scaled)
Distance de M. mais l'anomalie est divis√©e par la d√©viation standard moyenne (
une dev stand par dimension).

### Mahalanobis
Calcul de la distance plus complexe. En gros distance Euclidienne avec des
m√©canisme d'att√©nuation des effets des corr√©lations entre certaines *features*.

### Mahalanobis (normed)
aka "Normalized Bayes classifier"
M√™me normalisation qu'Euclide.

### Nearest-neighbor (Mahalanobis)
Distance de Mahalanobis entre chaque vecteur et le vecteur test√©. L'anomalie
retourn√©e est la distance avec le NN.

### Neural-network (standard)
IMBITABLE.

### Neural-network (auto-assoc)
PAS MIEUX.

La structure de ce r√©seau de neurones est adapt√© √† la d√©tection d'anomalies,
contrairement au d√©tecteur pr√©c√©dent. En gros produit des nouveaux vecteurs
en accentuant le score d'anomalie des imposteurs.

### Fuzzy logic
On associe des intervalles de timing √† des ensembles "flous". Chaque √©l√©ment
peut partiellement appartenir √† plusieurs ensembles. 
On calcule l'appartenance la plus forte de chacune des features √† un ensemble.
L'anomalie est calcul√©e comme la moyenne de non appartenance au set auquel la
feature appartenait le plus fortement en moyenne de toutes les features.

### Outlier-counting (z-score)
aka "Statistical technique" => pour chaque feature diff√©rence vecteur moyen /
vecteur test√©, le tout divis√© par la deviation standard de chaque feature.

Anomalie = combien de z-score des features on d√©pass√© un certain seuil.

### SVM (one-class)
Imbitable

### k-means
Clustering puis est-ce que le vecteur peut appartenir √† un de ces clusters.

## Evaluation methodology
### Training and testing the detectors
Sur les 51 utilisateurs, on entraine le mod√®le √† en consid√©rer un comme
l'authentique, et on d√©signe les autres comme des imposteurs.

On teste le mod√®le contre la moiti√© des tentatives utilisateurs pour faire un
*user score*, c√†d un score de d√©tection des anomalies sur des samples qui n'en
sont pas.

Puis on le teste sur autant de samples d'imposteurs, pour obtenir un score
d'anomalie vs des "vrais" imposteurs.

Une it√©ration par utilisateur est faite, avec chaque utilisateur √©tant consid√©r√©
comme √©tant l'utilisateur authentique.

### Calculating detector performance
ROC curve (Receiver Operating Characteristic = tracer FPR (False Positive Rate
== False Alarm Rate == fall-out) vs TPR (True Positive Rate == Hit rate ), 
m√©trique classique pour l'√©valuation des performance des d√©tecteurs.
[Voir cette page]
(https://en.wikipedia.org/wiki/Receiver_operating_characteristic)

Pour d√©limiter entre les anomalies et les utilisateurs authentiques, il faut
d√©finir un *seuil* qui fixe un taux d'anomalie √† partir duquel on d√©clenche une
alarme.üóØ

Le rapport entre FPR et TPR est donc situ√© sur un continuum dont la valeur
finale va d√©pendre du seuil. Les m√©thodes choisies pour fixer le seuil sont :

* *equal-error rate* : les taux d'erreur sont √©gaux
* *zero-miss false alarm rate* : le *miss rate* est √† z√©ro **et** on prend la
valeur minimale de *false alarm rate* en respectant cette contrainte.

## Results and analysis
### Finding the top-performing detectors
J'ai rien compris √† cette partie. En gros un d√©tecteur n'est un *top
performer* que s'il est comp√©titif avec le *best performer* ? 

### Detector performance comparison
Aucun des d√©tecteurs n'atteint la performance requise par le standard europ√©en.

Globalement les d√©tecteurs qui performent le mieux emploient une pond√©ration des
*features* qui varient beaucoup, avec une m√©thode comme Mahalanobis, ou la 
distance de Manhattan pond√©r√©e.

La m√©thode du Nearest Neighbor en distance de Mahalanobis est la seule m√©thode
qui performe bien sur les deux m√©thodes de s√©lection du seuil.

Globalement l'ensemble des m√©thodes √©valu√©e est nettement divis√© entre la moiti√©
qui performe le mieux, et la moiti√© qui est bof.

## Discussion and future work
### Shared data and methods
Manque criant de donn√©es et de m√©thodes partag√©es dans ce champ de la recherche.

### Extensions to the evaluation method
On peut utiliser la base de donn√©es pour √©valuer autre chose que la d√©tection
d'anomalie.

On peut aussi exclure certaines *features* du set de donn√©es qui est tr√®s 
complet, de mani√®re √† observer la performance des d√©tecteurs avec et sans ces 
*features*.

### Detector variability across data sets
Des variations de perf majeurs peuvent r√©sulter diff√©rences mineures dans

* l'impl√©mentation des d√©tecteurs;
* la donn√©e d'entra√Ænement;
* la m√©thodologie d'√©valuation (seuil de d√©tection)

> Keystroke-dynamics is a sensitive instrument in a noisy domain.

On se retrouve avec des conclusions inverses sur les comparaisons d'algo que
les publications sur lesquelles elles s'appuient.

## Synth√®se

Il existe une multitude de publications sur l'identification d'un utilisateur
sur la base de sa mani√®re d'√©crire au clavier. Chaque publication cherche √†
d√©terminer quels algorithmes sont les plus performants, mais a aussi son propre
protocole d'acquisition des donn√©es, rendant la comparaison directe de deux
algorithmes impossible.

Cela pose un probl√®me car le standard europ√©en pour les syst√®mes de contr√¥le 
d'acc√®s est tr√®s exigeant : FRR <= 1%, FAR <= 0.001 % (EN-50133-1).

Avec les techniques de biom√©trie comportementale actuelles, ce standard est 
assez loin d'√™tre respect√©. Cela rend impossible la commercialisation de 
dispositifs permettant d'identifier une personne uniquement sur la mani√®re dont 
elle frappe au clavier.

Cette publication propose d'apporter √† la recherche une base de donn√©es et une
m√©thodologie reproductible, afin de permettre les comparaisons entre les
algorithmes utilis√©s par les chercheurs en *keystroke dynamics*.

La m√©thodologie propos√©e se limite √† l'identification d'une personne tapant un
mot de passe fixe, et les algorithmes de classifications qu'on appelle les
**d√©tecteurs d'anomalie**. Ces algorithmes n'identifient pas une personne parmi
plusieurs possibles mais identifient si la personne est bien la bonne personne
ou non. On a donc une classification de la donn√©e entre deux classes :

* Utilisateur authentique ;
* Imposteurs.

M√™me si cette m√©thodologie limite quelque peu les comparaisons √† une partie
sp√©cifique de la recherche dans le champ des *keystroke dynamics*, les auteurs
soulignent que la base de donn√©es pourrait √™tre utilis√©e pour des algorithmes de
classification √† plusieurs classes (identifier un utilisateur parmi plusieurs).
Si la donn√©e rendue publique ne comporte que des utilisateurs tapant un mot de
passe fixe, les auteurs argumentent que la m√©thodologie pourrait √™tre
g√©n√©ralis√©e pour acqu√©rir la donn√©e d'utilisateurs tapant du texte libre.

La publication soul√®ve un probl√®me de taille pour l'√©valuation des performances
de syst√®mes d'identification par frappe au clavier : un utilisateur va rentrer
son mot de passe plusieurs fois pour constituer un profil reconnaissable par
l'algorithme de d√©tection. Cependant, pour v√©rifier si l'algorithme est bien
capable de discriminer un imposteur essayant de se connecter en utilisant le 
m√™me mot de passe, il faut des donn√©es de personnes tapant le m√™me mot de passe,
ce qui n'est pas une donn√©e dont on dispose dans la vraie vie. Par cons√©quent
constituer une base de donn√©es o√π tous les √©chantillons de tous les utilisateurs
sont bas√©s sur le m√™me mot de passe est artificiel, mais offre une facilit√©
d'√©valuation des performances des algorithmes. Il faut arriver √† √©valuer les
performances de ces algorithmes, tout en √©vitant d'entra√Æner un mod√®le qui
surinterpr√™te sur le mot de passe retenu.

La publication apporte aussi des orientations sur les protocole d'acquisition de
la donn√©e, notamment en soulignant l'utilisation d'une horloge externe pour
une plus grande pr√©cision, et qu'ils ont choisi de ne pas retenir les tentatives
erronn√©es dans la base de donn√©es. Les auteurs parlent aussi de la mise en
forme des donn√©es, soit le processus de prendre une tentative d'identification
d'un utilisateur et le transformer en √©chantillon exploitable par le syst√®me.

Une fois la m√©thodologie d√©taill√©e et le protocole de comparaison mis en place,
les auteurs pr√©sentent les diff√©rents algorithmes compar√©s. Ils en expliquent le
principe g√©n√©ral et r√©f√©rencent les publications qui les ont impl√©ment√©s.

Enfin, la publication utilise une m√©thode statistique pour d√©terminer quels sont
les algorithmes les plus performants suivant deux crit√®res : la limitation des
faux positifs (FAR) avec  et la limitation des faux n√©gatifs (FRR).

L'apport de cette publication est plus la m√©thodologie de comparaison que les
r√©sultats de la comparaison qui sont amen√©s √† √©voluer avec la recherche. On
retiendra notamment que dans la d√©tection entre utilisateur authentique et 
imposteur, le seuil de discrimination et la performance vis√©e peut avoir une
forte influence. De m√™me, le protocole d'acquisition des donn√©es peut introduire
une tr√®s forte variation des performances d'un algorithme. Donc pour √©valuer
la performance d'un algorithme, il faudrait pouvoir le tester sur plusieurs 
bases de donn√©es diff√©rentes.

## Ce que je retiens

Attention au jargon et √† ses d√©finitions:
False-alarm Rate = False Rejection Rate
Miss rate = False Acceptance rate.

Attention :
Choix du pw en fonction de ce qu'il se faisait avant fait qu'on reste sur des
mdp des ann√©es 2000, trop faibles.
Travailler sur un set de donn√©es avec des utilisateurs qui rentrent le m√™me mot
de passe ne correspond pas vraiment √† la r√©alit√©, mais √ßa simplifie l'√©valuation
des performances.

M√©thode d'acquisition de la donn√©e :
Utilisation d'une horloge de r√©f√©rence externe pour augmenter la pr√©cision des
timestamps. Alors que nous on aimerait travailler uniquement sur de la
keystroke dynamics sans rajouter de hardware. (Apr√®s ils √©taient sur du Windows)

M√©thodologie de constitution d'un *timing vector* floue mais peut nous mettre
dans la bonne direction.

Liste des impl√©mentations des d√©tecteurs : explication succincte et relativement
compr√©hensible des algorithmes utilis√©s.

L'apport de cette publication est plus la m√©thodologie de comparaison que les
r√©sultats de la comparaison elle-m√™me.

Globalement le champ de la recherche est incapable de fournir une √©valuation des
performances des algos utilis√©s qui soit directement comparable d'une
publication √† l'autre.

Les conditions d'acquisition, de formalisation de la donn√©e, d'impl√©mention des
algos, d'√©valuation des performances sont autant de variables dont la **moindre
modification** peut entra√Æner une divergeance des performances finales. Il faut
donc r√©fl√©chir √† des conditions qui se rapprochent de la r√©alit√© plut√¥t que de
conditions de laboratoires pour √©valuer des syst√®mes amen√©s √† √™tre utilis√©s
pour de vrai.







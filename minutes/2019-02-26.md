# Phase prototypage Réunion

Intégration : volonté louable mais se concentrer sur quelque chose de reproductible et testable.

## ML et argumentation scientifique

Un pôle du travail qui **doit** tourner autour de la reproduction de résultats précédents (pour montrer qu'on s'inscrit dans une méthodo qu'on sort pas de notre cul), et de nos propres résultats (pour que quelqu'un d'autre puisse les reproduire.

* Création d'une librairie / module qui permettrait de prendre plein de jeux de tests en entrée pour pouvoir voir ressortir les résultats que nous aurions obtenus.

Problème : on va faire de la *novelty detection*, ce qui est assez différent de l'approche par catégorisation de plusieurs utilisateurs. On va manquer de comparaisons dans l'état de l'art : la tendance est plutôt à faire de la catégorisation parce que c'est plus simple du point de vue des jeux de données, et c'est plus facile à mettre des résultats en évidence.
Il va falloir qu'on utilise les mêmes données et à peu près les mêmes métriques tout en partant sur une problématique complètement différente. Notre approche est plus près d'un cas d'application réel mais aussi moins simple à tester.

Il faut qu'on collecte des données qui seront aussi de notre cru.
Récolter des données dans un premier temps : soft pour créer des tableaux de tentatives.
Keylogger => librairie si on veut que ça soit utilisé par deux modules PAM

Problème définition du protocole de test : Comment on fait pour montrer que l'algo marche bien ? Ça a l'air assez "simple" du point de vue du *machine learning* : on entraine sur une partie des données qu'on a disposition, et on le teste sur le reste. On refait plusieurs fois avec un découpage différent sur le même jeu de données. On peut faire ça sur plusieurs jeux de données.

Affinement / pondération des paramètres : à quel moment on s'arrête ?

Par contre durant la recherche des meilleurs paramètres : il faudra à chaque fois tester les paramètres sur plusieurs jeux de données différents.

Résultats à exprimer en fonction des métriques classiques de la biométrie (FAR, FRR, etc... cf. rapport).

L'argumentation doit tourner autour de la qualité du modèle, et ses performances mesurées avec les métriques classiques de la biométrie. Ces métriques exprimeront forcément la qualité de la *sécurité* du modèle : en fonction des faux positifs, faux négatifs.

**Attention** : la balance faux positifs / faux négatifs est au moins partiellement un **choix** de notre part. Il y a des normes pour les systèmes d'authentification (cf. rapport), mais au-delà il faut voir aussi ce qu'on valorise d'un point de vue **expérience utilisateur**.

Comment on rend compte du fonctionnement du modèle de manière graphique ?Problème de Rodolphe avec *Decision function* et représentation graphique des données. C'est aussi un problème pour la soutenance : support visuel doit mettre en évidence la délimitation des données.

Étude dans le temps : comment on évalue l'évolution du modèle ?
Sécurité modèle évolutif : déplacer le modèle progressivement.

## Considération de sécurité reverse engineering du modèle

Rajouter un aspect évaluation de la sécurité du modèle en partant du modèle déchiffré, est-ce que c'est simple de tromper le système d'authentification.

Est-ce qu'on arrive à déduire des timings du modèle déchiffré, et est-ce qu'on arrive a entraîner une autre IA pour tromper le modèle ?

Utilisation éventuelle de GAN pour mettre en concurrence le modèle de protection et le modèle attaquant.

Sécurité du modèle : secret de chiffrement du modèle stocké côté root.
Comment faire pour que le temps soit constant malgré exécution séquentielle du 2FA (dans la stack PAM) ? Éviter les attaques par timing.

## Intégration

Utilisateur : 10 enregistrements pour créer un modèle personnalisé.

Modèle entraîné : export / compilation ? Sérialization, réutilisation dans un autre soft ? Comment on fait pour le réentrainer progressivement ?

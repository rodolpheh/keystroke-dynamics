{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-class SVM\n",
    "\n",
    ">One-class SVM is an unsupervised algorithm that learns a decision function for novelty detection: classifying new data as similar or different to the training set.\n",
    "\n",
    "[Scikit-learn documentation](https://scikit-learn.org/stable/auto_examples/svm/plot_oneclass.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the math ! Do the math !\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Matplotlib imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# IPython widgets and display\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Every scikit-learn import\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import ParameterGrid, GridSearchCV\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "from Data import Data\n",
    "from ResultTable import ResultTable\n",
    "from Model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data()\n",
    "\n",
    "columns = [\"rrTime\" + str(index) for index in range(15)]\n",
    "columns.extend([\"ppTime\" + str(index) for index in range(15)])\n",
    "columns.extend([\"rpTime\" + str(index) for index in range(15)])\n",
    "columns.extend([\"prTime\" + str(index) for index in range(15)])\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(-10, 10, 500), np.linspace(-10, 10, 500))\n",
    "Xpred = np.array([xx.ravel(), yy.ravel()] + [np.repeat(0, xx.ravel().size) for _ in range(58)]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the user tested in the One-Class SVM. The user's index will be used to build a training and testing set solely containing samples from the chosen user.\n",
    "\n",
    "The rest of the data will be used as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUserData(userId):\n",
    "    # We select the data of the user\n",
    "    studiedUserData = data.table.loc[(data.table[\"user_id\"] == userId), columns].astype('float')\n",
    "    # We select the rest of the data. It will be used as outliers.\n",
    "    otherUsersData = data.table.loc[(data.table[\"user_id\"] != userId), columns].astype('float')\n",
    "    \n",
    "    return [studiedUserData, otherUsersData]\n",
    "\n",
    "\n",
    "def buildData(userId, trainSize):\n",
    "    studiedUserData, otherUsersData = getUserData(userId)\n",
    "\n",
    "    # Use a ShuffleSplit to select a random subset of the user's data\n",
    "    splitter = ShuffleSplit(n_splits=1, train_size=trainSize, test_size=None)\n",
    "    X_train_indices, X_test_indices = list(splitter.split(studiedUserData))[0]\n",
    "\n",
    "    X_train = studiedUserData.iloc[X_train_indices]\n",
    "    X_test = studiedUserData.iloc[X_test_indices]\n",
    "\n",
    "    # Abnormal data (used only for prediction)\n",
    "    X_outliers = otherUsersData\n",
    "    \n",
    "    return [X_train, X_test, X_outliers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and training the SVM\n",
    "\n",
    "This block will define a `learn(...)` function that can be used on built data from the `buildData(...)` function.\n",
    "\n",
    "### The Pipeline\n",
    "\n",
    "The pipeline used includes a MinMaxScaler and an OneClassSVM classifier.\n",
    "\n",
    "### MinMaxScaler parameters\n",
    "\n",
    "According to https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf data for SVM should be linearly scaled to the range \\[0, 1\\] or \\[-1, 1\\] Also, the scaler should be fitted on the training data only before transforming the rest of the data.\n",
    "\n",
    "### OneClassSVM parameters\n",
    "\n",
    "In the case where there is much more samples than features, rbf kernel is usually the way to go. Setting the kernel to linear raised A LOT of false positive on abnormal data (~97% !!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPipeline():\n",
    "    # Create the scaler and the classifier\n",
    "    scaler = MinMaxScaler((0, 1))\n",
    "    classifier = OneClassSVM(kernel=\"linear\")\n",
    "    \n",
    "    # Create the pipeline\n",
    "    pipeline = Pipeline([('scaler', scaler), ('classifier', classifier)])\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def learn(train, nu=0.01, gamma=0.01):\n",
    "    pipeline = createPipeline()\n",
    "    pipeline.set_params(classifier__nu=nu, classifier__gamma=gamma)\n",
    "    \n",
    "    # Fit the data\n",
    "    pipeline.fit(train)\n",
    "    \n",
    "    return pipeline\n",
    "    \n",
    "    \n",
    "def scoreFull(pipeline, train, test, outliers):\n",
    "    \n",
    "    # Predict on all data\n",
    "    y_pred_train = pipeline.predict(train)\n",
    "    y_pred_test = pipeline.predict(test)\n",
    "    y_pred_outliers = pipeline.predict(outliers)\n",
    "    \n",
    "    TP = y_pred_test[y_pred_test == 1].size\n",
    "    TN = y_pred_outliers[y_pred_outliers == -1].size\n",
    "    FP = y_pred_outliers[y_pred_outliers == 1].size\n",
    "    FN = y_pred_test[y_pred_test == -1].size\n",
    "    \n",
    "    return {\n",
    "        \"TP\": TP,\n",
    "        \"TN\": TN,\n",
    "        \"FP\": FP,\n",
    "        \"FN\": FN,\n",
    "        \"FNT\": y_pred_train[y_pred_train == -1].size,\n",
    "        \"precision\": TP / (TP + FP + 0.0001),\n",
    "        \"recall\": TP / (TP + FN),\n",
    "        \"f1\": (2 * TP) / (2 * TP + FP + FN),\n",
    "        \"clf\": pipeline,\n",
    "        \"accuracy\": (TP + TN) / (TP + TN + FP + FN)\n",
    "    }\n",
    "\n",
    "\n",
    "def score(pipeline, train, test):\n",
    "    \n",
    "    # Predict on all data\n",
    "    y_pred_train = pipeline.predict(train)\n",
    "    y_pred_test = pipeline.predict(test)\n",
    "    \n",
    "    TP = y_pred_test[y_pred_test == 1].size\n",
    "    FN = y_pred_test[y_pred_test == -1].size\n",
    "    \n",
    "    return {\n",
    "        \"TP\": TP,\n",
    "        \"FN\": FN,\n",
    "        \"FNT\": y_pred_train[y_pred_train == -1].size,\n",
    "        \"recall\": TP / (TP + FN),\n",
    "        \"clf\": pipeline\n",
    "    }\n",
    "\n",
    "\n",
    "def recall_score(y_true, y_pred):\n",
    "    TP = y_pred[y_pred == 1].size\n",
    "    FN = y_pred[y_pred == -1].size\n",
    "    return TP / (TP + FN)\n",
    "\n",
    "recall_scorer = make_scorer(recall_score, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn and score one user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = widgets.Dropdown(options=data.users, value=1, description='User:', disabled=False)\n",
    "display(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_outliers = buildData(user.value, 10)\n",
    "model = learn(X_train, nu=0.9500000000000001, gamma=1e-08)\n",
    "results = scoreFull(model, X_train, X_test, X_outliers)\n",
    "\n",
    "print(\"False negative on training data : {0}/{1}, {2:.3f}%\".format(results[\"FNT\"],\n",
    "                                                                  X_train.shape[0],\n",
    "                                                                  results[\"FNT\"] / X_train.shape[0] * 100))\n",
    "print(\"False negative on testing data : {0}/{1}, {2:.3f}%\".format(results[\"FN\"],\n",
    "                                                                 X_test.shape[0],\n",
    "                                                                 results[\"FN\"] / X_test.shape[0] * 100))\n",
    "print(\"False positive on abnormal data : {0}/{1}, {2:.3f}%\".format(results[\"FP\"],\n",
    "                                                                  X_outliers.shape[0],\n",
    "                                                                  results[\"FP\"] / X_outliers.shape[0] * 100))\n",
    "\n",
    "print(\"precision: {},\\nrecall: {},\\nf1: {}\".format(results[\"precision\"], results[\"recall\"], results[\"f1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the decision function\n",
    "\n",
    "Apparently, this is not very significative. When the 60-dimensional points are projected on a 2-dimensional plane, abnormal data overlap the area included inside the decision function.\n",
    "\n",
    "Plus, I do have issues for plotting the frontiers.\n",
    "\n",
    "To have a more significative representation, we should search for the wo most important features by using features selection techniques as PCA, ANOVA or factor analysis. Then get the decision function for these features and plot it. The overlap should be minimized and the plot more representative.\n",
    "\n",
    "This cell is disabled by setting it to raw text. Turn it back to Python to use it."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Z = results[\"clf\"].decision_function(Xpred)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.title(\"Novelty Detection\")\n",
    "plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu)\n",
    "a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')\n",
    "#plt.contourf(xx, yy, Z, levels=np.linspace(0, Z.max(), 7), colors='palevioletred')\n",
    "\n",
    "s = 40\n",
    "b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white', s=s, edgecolors='k')\n",
    "b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='blueviolet', s=s,\n",
    "                 edgecolors='k')\n",
    "c = plt.scatter(X_outliers[:20, 0], X_outliers[:20, 1], c='gold', s=s,\n",
    "                edgecolors='k')\n",
    "plt.axis('tight')\n",
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-5, 5))\n",
    "plt.legend([a.collections[0], b1, b2, c],\n",
    "           [\"learned frontier\",\n",
    "            \"training observations\",\n",
    "            \"new regular observations\",\n",
    "            \"new abnormal observations\"],\n",
    "           loc=\"upper left\", prop=matplotlib.font_manager.FontProperties(size=11))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching the best parameters with cross-validation\n",
    "\n",
    "To find the best hyperparameters of our SVM with cross-validation, we need to know what should be our scoring function. Accuracy is by default the most simple score for parameters optimization. But in some cases, we may resort to precision, recall or f1-score.\n",
    "\n",
    "[This article](https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9) sums up the how and why of those differents scores.\n",
    "\n",
    "In our case, a false negative does not bring any harm to our user, only some annoyances (of course, there is a limit in this annoyance). However, a false positive is to be avoided as it would means that the user's computer is compromised. At first glance, using the precision may be the way to go.\n",
    "\n",
    "However, computing the precision would need to know the rate of false positive, which is not available in a case where we don't have impostors data.\n",
    "\n",
    "F-1 score is particularly useful when we need a balance between the precision and the recall, and when there is an uneven class distribution, which is our case when we make our predictions on all the impostor data.\n",
    "\n",
    "### Simple cross-validation\n",
    "\n",
    "**This only applies when we have impostor data and test data to feed to the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'gamma' : np.logspace(-9, 3, 13),\n",
    "        'nu' : np.linspace(0.01, 0.99, 99)}\n",
    "\n",
    "usersCounts = np.array(data.usersCounts)\n",
    "resultsTable = ResultTable()\n",
    "\n",
    "clear_output(wait=False)\n",
    "display(resultsTable)\n",
    "\n",
    "for user in usersCounts[(usersCounts[:, 1] > 30)]:\n",
    "    X_train, X_test, X_outliers = buildData(user[0], 10)\n",
    "\n",
    "    biggestF1 = 0\n",
    "    biggestF1Parameters = None\n",
    "    bestResults = None\n",
    "\n",
    "    for hyperparams in ParameterGrid(grid):\n",
    "        model = learn(X_train, nu=hyperparams[\"nu\"], gamma=hyperparams[\"gamma\"])\n",
    "        results = scoreFull(model, X_train, X_test, X_outliers)\n",
    "\n",
    "        if results[\"f1\"] > biggestF1:\n",
    "            biggestF1 = results[\"recall\"]\n",
    "            biggestF1Parameters = hyperparams\n",
    "            bestResults = results\n",
    "            \n",
    "    shortParams = {\n",
    "        'gamma': bestResults[\"classifier__gamma\"],\n",
    "        'nu': bestResults[\"classifier__nu\"]\n",
    "    }\n",
    "    \n",
    "    resultsTable.appendResults(user[0], results, shortParams)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    display(resultsTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better cross-validation\n",
    "\n",
    "Reference for this part : *Eude, T & Chang, Chuan. (2017). One-class SVM for biometric authentication by keystroke dynamics for remote evaluation. Computational Intelligence. 34. 10.1111/coin.12122.*\n",
    "\n",
    "![Optimization protocol](ocsvm-optimization.png)\n",
    "\n",
    "The positive data is split (as we want, in our case we may limit it to ten samples to simulates our use case). The first part constitute our training data whereas the second is mixed with impostor data and is used later in the evaluation of the model.\n",
    "\n",
    "In this figure, the Data 1 used for the cross-validation (made by a subset of the positive data) is folded 5 times and used in a classic cross-validation algorithm where the recall is used as the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersCounts = np.array(data.usersCounts)\n",
    "resultsTable = ResultTable()\n",
    "\n",
    "clear_output(wait=False)\n",
    "display(resultsTable)\n",
    "\n",
    "for user in usersCounts[(usersCounts[:, 1] > 40)]:\n",
    "\n",
    "    studiedUserData, otherUsersData = getUserData(user[0])\n",
    "\n",
    "    # Use a ShuffleSplit to select 80% of the user's data\n",
    "    splitter = ShuffleSplit(n_splits=1, train_size=0.5, test_size=None)\n",
    "    dataOneIndices, dataTwoIndices = list(splitter.split(studiedUserData))[0]\n",
    "\n",
    "    dataOne = studiedUserData.iloc[dataOneIndices]\n",
    "    dataTwoPositive = studiedUserData.iloc[dataTwoIndices]\n",
    "\n",
    "    splitter = ShuffleSplit(n_splits=1, train_size=10, test_size=None)\n",
    "    trainIndices, testIndices = list(splitter.split(dataOne))[0]\n",
    "\n",
    "    train = dataOne.iloc[trainIndices]\n",
    "    test = dataOne.iloc[testIndices]\n",
    "\n",
    "    grid = {'gamma' : np.logspace(-9, 3, 13),\n",
    "            'nu' : np.linspace(0.01, 0.99, 99)}\n",
    "\n",
    "    bestRecall = 0\n",
    "    bestRecallParameters = None\n",
    "    bestRecallResults = None\n",
    "\n",
    "    for hyperparams in ParameterGrid(grid):\n",
    "        model = learn(train, nu=hyperparams[\"nu\"], gamma=hyperparams[\"gamma\"])\n",
    "        results = score(model, train, test)\n",
    "\n",
    "        if results[\"recall\"] > bestRecall:\n",
    "            bestRecall = results[\"recall\"]\n",
    "            bestRecallParameters = hyperparams\n",
    "            bestRecallResults = results\n",
    "\n",
    "    results = scoreFull(bestRecallResults[\"clf\"], train, dataTwoPositive, otherUsersData)\n",
    "\n",
    "    resultsTable.appendResults(user[0], results, bestRecallParameters)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    display(resultsTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersCounts = np.array(data.usersCounts)\n",
    "resultsTable = ResultTable()\n",
    "\n",
    "clear_output(wait=False)\n",
    "display(resultsTable)\n",
    "\n",
    "for user in usersCounts[(usersCounts[:, 1] > 40)]:\n",
    "\n",
    "    studiedUserData, otherUsersData = getUserData(3)\n",
    "\n",
    "    # Use a ShuffleSplit to select 80% of the user's data\n",
    "    splitter = ShuffleSplit(n_splits=1, train_size=0.8, test_size=None)\n",
    "    dataOneIndices, dataTwoIndices = list(splitter.split(studiedUserData))[0]\n",
    "\n",
    "    dataOne = studiedUserData.iloc[dataOneIndices]\n",
    "    dataTwoPositive = studiedUserData.iloc[dataTwoIndices]\n",
    "\n",
    "    pipeline = createPipeline()\n",
    "    gs = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid={\n",
    "            \"classifier__nu\": np.linspace(0.01, 0.99, 99),\n",
    "            \"classifier__gamma\": np.logspace(-9, 3, 13)\n",
    "        },\n",
    "        cv=5,\n",
    "        scoring=recall_scorer\n",
    "    )\n",
    "\n",
    "    gs.fit(dataOne, list([1 for i in range(len(dataOne.index))]))\n",
    "    params = gs.best_params_\n",
    "    shortParams = {\n",
    "        'gamma': gs.best_params_[\"classifier__gamma\"],\n",
    "        'nu': gs.best_params_[\"classifier__nu\"]\n",
    "    }\n",
    "\n",
    "    results = scoreFull(gs.best_estimator_, dataOne, dataTwoPositive, otherUsersData)\n",
    "\n",
    "    resultsTable.appendResults(user[0], results, shortParams)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    display(resultsTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersCounts = np.array(data.usersCounts)\n",
    "resultsTable = ResultTable()\n",
    "\n",
    "clear_output(wait=False)\n",
    "display(resultsTable)\n",
    "\n",
    "for user in usersCounts[(usersCounts[:, 1] > 40)]:\n",
    "\n",
    "    studiedUserData, otherUsersData = getUserData(3)\n",
    "\n",
    "    # Use a ShuffleSplit to select 80% of the user's data\n",
    "    splitter = ShuffleSplit(n_splits=1, train_size=0.8, test_size=None)\n",
    "    dataOneIndices, dataTwoIndices = list(splitter.split(studiedUserData))[0]\n",
    "\n",
    "    dataOne = studiedUserData.iloc[dataOneIndices]\n",
    "    dataTwoPositive = studiedUserData.iloc[dataTwoIndices]\n",
    "\n",
    "    model = Model()\n",
    "    gs = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid={\n",
    "            \"classifier__nu\": np.linspace(0.01, 0.99, 99),\n",
    "            \"classifier__gamma\": np.logspace(-9, 3, 13)\n",
    "        },\n",
    "        cv=5,\n",
    "        scoring=recall_scorer\n",
    "    )\n",
    "\n",
    "    gs.fit(dataOne, list([1 for i in range(len(dataOne.index))]))\n",
    "    params = gs.best_params_\n",
    "    shortParams = {\n",
    "        'gamma': gs.best_params_[\"classifier__gamma\"],\n",
    "        'nu': gs.best_params_[\"classifier__nu\"]\n",
    "    }\n",
    "\n",
    "    results = scoreFull(gs.best_estimator_, dataOne, dataTwoPositive, otherUsersData)\n",
    "\n",
    "    resultsTable.appendResults(user[0], results, shortParams)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    display(resultsTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "According to this page : https://stats.stackexchange.com/questions/192530/metrics-for-one-class-classification\n",
    "\n",
    "According to Lee, Wee Sun, and Bing Liu. [\"Learning with positive and unlabelled examples using weighted logistic regression.\"](https://www.aaai.org/Papers/ICML/2003/ICML03-060.pdf) ICML. Vol. 3. 2003. (creators of One-Class SVM)\n",
    "\n",
    "The best formula for the scoring function may be :\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{r^2}{\\Pr(f(x)=1)}\n",
    "\\end{equation*}\n",
    "\n",
    "$r$ is the *recall* and $Pr(f(x)=1)$ is the ratio of *true positives* (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
